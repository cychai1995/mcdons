---
model: # FPN 50 configuration
  image_width: 640
  image_height: 480
  desp_dim: 12
  FPN_pretrained: 'YOUR_PATH_FPN50/fpn50.weight'
  load_FPN_pretrained: true
  use_class: true # Add very tiny classification head
  class_dims: [64,64,8]
  use_fp16: true          # Use FP16 in training
  split_model: false # Split model into two GPU device
  net_module: 'FPN_center'
  gpu_seq: ['cuda:0','cuda:0']
  split_desp: false
  s_seg_class_only: false
  learn_center: false # center_dict is fixed or learnable ?
  center_dict: # Leave empty for randomly generated center ~N(0,1) {cls: [values]}
  center_classes: [1,2,3,4,5,6,7] #


sampling: # xxx_n: maximum num of elements in xxx from sampling, but not guarantee exactly the same number
  separate_match_non_match_sampling: false
  A_A2_only: true
  color_jit: true
  color_jit_range: [0.5, 0.5, 0.5]
  match_frame_near: 10 # +-10 image frame for finding match points
  point_distance_check: 0.002 # unit: meter
  dual_projection_check: true # A->A2 A2->A, more precise correspondence
  # Random Background Texture
  randomize_background: true # Use synthetic background
  texture_base: 'YOUR_PATH_VOC2012/VOCdevkit/VOC2012'
  texture_dirs: ['JPEGImages']
  bk_random_prob: 0.5
  # Multiple patch (matching_class_set_n+non_matching_class_set_n<=len(train_data['object_classes']))
  matching_class_set_n: 2 # Multiple Object Sampling
  non_matching_class_set_n: 1 # Multiple Object Sampling, If A_A2_only is true than it is not used
  non_match_kernel: 3
  #  # For Patch Location Sampling V0
  #  noise_max_u: 150 #Pixel
  #  noise_max_v: 50 #Pixel


  # For random scaling, translation and rotation (Abbr. rsr)
  random_scale_rotate: true
  scale_range_min: 0.7      # 0.6 1.0 +- scale_range
  scale_range_max: 1      #  1.1 1.0 +- scale_range
  angle_range: 180      # +- 180 degree
  translation_range: 0 #  +-0.3, Rotated image size * translation_range
  # For classification(segmentation) points sampling
  segmentation_n: 50000 # for each class including background, Balanced, at most segmentation_n for each class
#
  # For Pixel Distance Evaluation
  pix_dist_match_n: 150
  # Pair Points (For contrastive loss)
  balanced_match: True # True may reduce the number, but balanced
  match_n: 50000 # Maximum for both Non-balanced and Balanced
  background_n_per_class: 1000 # FG <-> BG contrastive
  background_same_n: 1000
  # Hard Negative Source pts sampling
  hard_negative_match: 180
  hard_negative_AB_n: 120
  mask_max_hard_negative: 50000 # Maximum mask sampling for hard negative



##   For Pixel Distance Evaluation
#  pix_dist_match_n: 300
#  # Pair Points (For contrastive loss)
#  balanced_match: True # True may reduce the number, but balanced
#  match_n: 10000 # Maximum for both Non-balanced and Balanced
#  background_n_per_class: 1000 # FG <-> BG contrastive
#  background_same_n: 1000
#  # Hard Negative Source pts sampling
#  hard_negative_match: 350
#  hard_negative_AB_n: 300
#  mask_max_hard_negative: 80000 # Maximum mask sampling for hard negative


criterion:
  A_A2_only: true
  l2normalize: false      # L2 Normalize raw descriptor output

  center_alpha: 0 # center (not pre center) loss multiplier
  clf_alpha: 0
  hard_alpha: 10 # Hard Negative loss multiplier
  match_alpha: 5 # Match loss multiplier
  triplet_alpha: 0
  match_alpha_obj: 5
  non_match_alpha_obj_bk: 1
  match_alpha_bk_bk: 0
  split_match: true
  use_hard_metric: false

  loss_norm_alpha: 0
  norm_space_max: 20

  # Contrastive
  contrastive_non_zero: true # For contrastive
  contrastive_sum_loss: false  # false for take mean
  match_m: 0
  background_m: 10
  background_consistent_m: 2
  match_only: false # match and background, ignore non_match_xx
  # Hard Negative for contrastive loss
  neg_closest_AA2: 5
  neg_closest_AB: 5
  neg_semi_hard_AB: false
  hard_negative_only_self: false

  neg_closest_pixel: 3
  # Triplet "Settings"
  no_triplet: true
  triplet_sum_loss: false
  triplet_pow2: true
  triplet_non_zero: true
  triplet_square_norm: false

  # New triplet pre_allocated center loss
  center_dist_diff: 5
#
#  # New Hard Metric Loss
#  hm_bg_points_start: 1000
#  hm_bg_points_max: 5000
#  hm_bg_increase_epoch: 1000 # increase 1000 per epoch
#
#  hm_fg_points_start: 200
#  hm_fg_points_max: 1000
#  hm_fg_increase_epoch: 500 # increase 1000 per epoch


training_param: # customized by user
  exp_name: 'baseline'        # Additional weight/progress file header
  # progress_path/[exp_name[epoch].tpdata
  # result_path/[exp_name][epoch].tpdata
  progress_path: 'YOUR_PATH_CHECKPOINT/progress' # Assign a path for the checkpoint
  result_path: 'YOUR_PATH_CHECKPOINT/result'  # Assign a path for the checkpoint


  dev: 0               # CUDA Dev ice Number, used when GPU is available
  worker: 12              # Dataloader Worker Number
  lrs: 'decay'            # Learning rate scheduler mode, choices=['epoch', 'loss', 'decay']
  lr_init: 0.0001          # Initial Learning rate for Torch lr Schedulers
  lr_decay_factor: 0.92
  lr_rates: # 0.0001, 0.00005,0.00002,0.00001]      # Learning Rates for epochs
  lr_epochs: # [10, 15, 20]     # Epochs for Learning Rate control
  lr_loss:                # None, for lrs='loss' mode

  # Learning Rate scheduler setting for centers if use_pre_centers=false
  ct_lrs: 'decay'
  ct_lr_init: 0.01          # Initial Learning rate for Torch lr Schedulers
  ct_lr_decay_factor: 0.92
  ct_lr_rates:      # Learning Rates for epochs
  ct_lr_epochs:     # Epochs for Learning Rate control
  ct_lr_loss:                # None, for lrs='loss' mode


  tr_bat: 1               # Training Batch Size
  ts_bat: 1               # Testing Batch Size
  max_epoch: 50          # Max Epoch for training

  optim: 'Adam'           # Optimizer, choices=['RMSprop', 'SGD', 'Adadelta', 'Adam']
  w_decay: 0.0001         # Weight decay parameter for Optimizer.
  se: 1                   # Save progress and do summary every se epoch
  res: false              # Restore progress and resume the training progress
  tps: 50                 # Restore Training Progress index(step)
  save_opt: false         # Save optimizer State Dict (Take some time !)?
  test: false             # Display Testing Result only!
  init_fin: false         # Weight Initialization by the fan-in size

evaluation:
  # Descriptor Evaluation
  device: 0
  test_set_n: 20 #40
  x_lim_multi: 6
  vis_pairs_n: 20          # Visualize first vis_pairs_n pairs
  # Evaluation Metric Setting
  pck_pix_list: [40,80,120]
  pck_alpha_list: [0.05,0.1,0.2,0.3] # PCK ??@XXX

  # Visualization Evaluation
  std_range: 3
  # Pixel Distance
  pixel_vis_scale: 20
  # Color <=> Label Mapping
  mask_vis_colors: # class_label:RGB, 0~1
    0: [0,0,0]
    1: [1,0,0]
    2: [0,1,0]
    3: [0,0,1]
    4: [0.5,0.5,0]
    5: [0.5,0,0.5]
    6: [0,0.5,0.5]
    7: [0.2,0.2,0.5]




train_data:
  base_dir: 'YOUR_PATH_DATASET/DataMCD-net'
  use_refined_depth: false
  depth_scale: 0.001 # 0.001 From real camera !!
  cam_mat: [[620.869, 0, 305.089], [0, 620.869, 248.527], [0, 0, 1]] # SR300 Cam Mat
  object_classes: [1,2,3,4,5,6,7] # 0 is background, ignore here
  class_match_sample_weight: [1,1,1,1,1,1,1] # Depends on the num of objects in each class or uniform ?
  class_to_object_ids:
    1: [65,66,67,68] #,69]
    2: [51,52,53,54,55] #,56]
    3: [11,12,13,14,15] #,16]
    4: [71,72,73,74,75,76] #,77]
    5: [42,43,44,45,46,47,48,49] #,41]
    6: [21,22,23,25,25,26,27] #,28]
    7: [31,32,33,34,35] #,36]
  classes_to_names:
    0: 'Background'
    1: 'Cube'
    2: 'Pepper'
    3: 'Fish Doll'
    4: 'Banana'
    5: 'Carton'
    6: 'Cap'
    7: 'Pringles'



  id_to_scenes: # ['Folder_name',Data_N(pose_number_each_scene)]
    11: [['0000', 60], ['0001', 60], ['0002', 60], ['0003', 60],['0004', 60]]
    12: [['0005', 60], ['0006', 60], ['0007', 60], ['0008', 60],['0009', 60]]
    13: [['0010', 60], ['0011', 60], ['0012', 60], ['0013', 60],['0014', 60]]
    14: [['0015', 60], ['0016', 60], ['0017', 60], ['0018', 60],['0019', 60]]
    15: [['0020', 60], ['0021', 60], ['0022', 60], ['0023', 60],['0024', 60]]
    #    16: [['0025', 60], ['0026', 60], ['0027', 60], ['0028', 60],['0029', 60]]
    21: [['0030', 60], ['0031', 60], ['0032', 60], ['0033', 60],['0034', 60]]
    22: [['0035', 60], ['0036', 60], ['0037', 60], ['0038', 60],['0039', 60]]
    23: [['0040', 60], ['0041', 60], ['0042', 60], ['0043', 60],['0044', 60]]
    24: [['0045', 60], ['0046', 60], ['0047', 60], ['0048', 60],['0049', 60]]
    25: [['0050', 60], ['0051', 60], ['0052', 60], ['0053', 60],['0054', 60]]
    26: [['0055', 60], ['0056', 60], ['0057', 60], ['0058', 60],['0059', 60]]
    27: [['0060', 60], ['0061', 60], ['0062', 60], ['0063', 60],['0064', 60]]
    #    28: [['0065', 60], ['0066', 60], ['0067', 60], ['0068', 60],['0069', 60]]
    31: [['0070', 60], ['0071', 60], ['0072', 60], ['0073', 60],['0074', 60]]
    32: [['0075', 60], ['0076', 60], ['0077', 60], ['0078', 60],['0079', 60]]
    33: [['0080', 60], ['0081', 60], ['0082', 60], ['0083', 60],['0084', 60]]
    34: [['0085', 60], ['0086', 60], ['0087', 60], ['0088', 60],['0089', 60]]
    35: [['0090', 60], ['0091', 60], ['0092', 60], ['0093', 60],['0094', 60]]
    #    36: [['0095', 60], ['0096', 60], ['0097', 60], ['0098', 60],['0099', 60]]
    #    41: [['0100', 60], ['0101', 60], ['0102', 60], ['0103', 60], ['0104', 60]]
    42: [['0105', 60], ['0106', 60], ['0107', 60], ['0108', 60], ['0109', 60]]
    43: [['0110', 60], ['0111', 60], ['0112', 60], ['0113', 60], ['0114', 60]]
    44: [['0115', 60], ['0116', 60], ['0117', 60], ['0118', 60], ['0119', 60]]
    45: [['0120', 60], ['0121', 60], ['0122', 60], ['0123', 60], ['0124', 60]]
    46: [['0125', 60], ['0126', 60], ['0127', 60], ['0128', 60], ['0129', 60]]
    47: [['0130', 60], ['0131', 60], ['0132', 60], ['0133', 60], ['0134', 60]]
    48: [['0135', 60], ['0136', 60], ['0137', 60], ['0138', 60], ['0139', 60]]
    49: [['0140', 60], ['0141', 60], ['0142', 60], ['0143', 60], ['0144', 60]]
    51: [['0145', 60], ['0146', 60], ['0147', 60], ['0148', 60], ['0149', 60]]
    52: [['0150', 60], ['0151', 60], ['0152', 60], ['0153', 60], ['0154', 60]]
    53: [['0155', 60], ['0156', 60], ['0157', 60], ['0158', 60], ['0159', 60]]
    54: [['0160', 60], ['0161', 60], ['0162', 60], ['0163', 60], ['0164', 60]]
    55: [['0165', 60], ['0166', 60], ['0167', 60], ['0168', 60], ['0169', 60]]
    #    56: [['0170', 60], ['0171', 60], ['0172', 60], ['0173', 60], ['0174', 60]]
    #    60: [['0175', 60], ['0176', 60], ['0177', 60], ['0178', 60], ['0179', 60]]
    #    61: [['0180', 60], ['0181', 60], ['0182', 60], ['0183', 60], ['0184', 60]]
    #    62: [['0185', 60], ['0186', 60], ['0187', 60], ['0188', 60], ['0189', 60]]
    #    63: [['0190', 60], ['0191', 60], ['0192', 60], ['0193', 60], ['0194', 60]]
    #    64: [['0195', 60], ['0196', 60], ['0197', 60], ['0198', 60], ['0199', 60]]
    65: [['0200', 60], ['0201', 60], ['0202', 60], ['0203', 60], ['0204', 60]]
    66: [['0205', 60], ['0206', 60], ['0207', 60], ['0208', 60], ['0209', 60]]
    67: [['0210', 60], ['0211', 60], ['0212', 60], ['0213', 60], ['0214', 60]]
    68: [['0215', 60], ['0216', 60], ['0217', 60], ['0218', 60], ['0219', 60]]
    #    69: [['0220', 60], ['0221', 60], ['0222', 60], ['0223', 60], ['0224', 60]]
    71: [['0225', 60], ['0226', 60], ['0227', 60], ['0228', 60], ['0229', 60]]
    72: [['0230', 60], ['0231', 60], ['0232', 60], ['0233', 60], ['0234', 60]]
    73: [['0235', 60], ['0236', 60], ['0237', 60], ['0238', 60], ['0239', 60]]
    74: [['0240', 60], ['0241', 60], ['0242', 60], ['0243', 60], ['0244', 60]]
    75: [['0245', 60], ['0246', 60], ['0247', 60], ['0248', 60], ['0249', 60]]
    76: [['0250', 60], ['0251', 60], ['0252', 60], ['0253', 60], ['0254', 60]]
  #    77: [['0255', 60], ['0256', 60], ['0257', 60], ['0258', 60], ['0259', 60]]

  sample_len: 200

test_data:
  scene_type: 'single'
  base_dir: 'YOUR_PATH_DATASET/DataMCD-net'
  use_refined_depth: false
  depth_scale: 0.001 # 0.001 From real camera !!
  cam_mat: [[620.869, 0, 305.089], [0, 620.869, 248.527], [0, 0, 1]] # SR300 Cam Mat
  object_classes: [1,2,3,4,5,6,7] # 0 is background, ignore here
  class_match_sample_weight: [1,1,1,1,1,1,1] # Depends on the num of objects in each class or uniform ?
  class_to_object_ids:
    1: [69]
    2: [56]
    3: [16]
    4: [77]
    5: [41]
    6: [28]
    7: [36]
  id_to_scenes: # ['Folder_name',Data_N(pose_number_each_scene)]
    16: [['0025', 60], ['0026', 60], ['0027', 60], ['0028', 60],['0029', 60]]
    28: [['0065', 60], ['0066', 60], ['0067', 60], ['0068', 60],['0069', 60]]
    36: [['0095', 60], ['0096', 60], ['0097', 60], ['0098', 60],['0099', 60]]
    41: [['0100', 60], ['0101', 60], ['0102', 60], ['0103', 60], ['0104', 60]]
    56: [['0170', 60], ['0171', 60], ['0172', 60], ['0173', 60], ['0174', 60]]
    69: [['0220', 60], ['0221', 60], ['0222', 60], ['0223', 60], ['0224', 60]]
    77: [['0255', 60], ['0256', 60], ['0257', 60], ['0258', 60], ['0259', 60]]
  sample_len: 200

test_single:
  frame_near: [10] # 1~frame_near
  total_classes: [1,2,3,4,5,6,7] # Replace match_classes sequentially
  result_name: 'pcktest_baseline.yaml'  # Store at training_param: training_param['result_path']/training_param['exp_name']/pck_test.yaml

#visual_data:
#  base_dir: 'YOUR_PATH_DATASET/DataMCD-net'
#  use_id: 300
#  id_to_scenes:
#    100: [['2011', 60]] # cube doll carton
#    200: [['1015', 60],['1016', 60]] # pringles
#    300: [['1011',60],['1012',60]] # dolls
#
#  frame_near: 5










